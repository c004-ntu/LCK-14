{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Base class for data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Detect language\n",
    "from langdetect import detect\n",
    "from deep_translator import GoogleTranslator\n",
    "from textwrap import wrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataIngestion:\n",
    "    \"\"\"\n",
    "    Class responsible for reading data files and performing initial data cleaning.\n",
    "    \"\"\"\n",
    "    def __init__(self, volumes_file_path:str, archives_file_path: str, \n",
    "                 articles_file_path: str, \n",
    "                 contents_file_path:str, \n",
    "                 authors_file_path:str, authors_articles_file_path:str):\n",
    "        self.volumes_file_path = volumes_file_path        \n",
    "        self.archives_file_path = archives_file_path\n",
    "        self.articles_file_path = articles_file_path\n",
    "        self.contents_file_path = contents_file_path\n",
    "        self.authors_file_path = authors_file_path\n",
    "        self.authors_articles_file_path = authors_articles_file_path\n",
    "\n",
    "    def load_data(self) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Loads the archives, articles, and contents DataFrames from MySQL\n",
    "        \"\"\"\n",
    "        print(\"[INFO] Loading data files...\")\n",
    "        volumes_df = self.mysql_writer.read_table_volumes()\n",
    "        archives_df = self.mysql_writer.read_table_archives(status=\"\")\n",
    "        articles_df = self.mysql_writer.read_table_articles(status=\"\")\n",
    "        contents_df = self.mysql_writer.read_table_contents()\n",
    "        authors_df = self.mysql_writer.read_table_authors()\n",
    "        authors_articles_df = self.mysql_writer.read_table_authors_articles()\n",
    "\n",
    "        return volumes_df, archives_df, articles_df, contents_df, authors_df\n",
    "\n",
    "    def clean_articles(self, articles_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Simple Data preprocessing.\n",
    "        \"\"\"\n",
    "        print(\"[INFO] Cleaning articles...\")\n",
    "        articles_df = articles_df.fillna('')\n",
    "\n",
    "        return articles_df\n",
    "    \n",
    "    def clean_article_authors(self, articles_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Clean authors\n",
    "        articles_df = articles_df.assign(author=articles_df['author'].fillna(''))\n",
    "        articles_df['author_clean'] = (articles_df['author']\n",
    "                                            .str.replace(r'\\.', '', regex=True)\n",
    "                                            .str.replace(r'\\s+', ' ', regex=True)  # This replaces multiple spaces with one\n",
    "                                            .str.strip()  # Remove leading/trailing spaces\n",
    "                                            )\n",
    "\n",
    "        return articles_df\n",
    "    \n",
    "    def clean_article_titles (self, articles_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Clean titles\n",
    "        articles_df = articles_df.assign(article_title=articles_df['article_title'].fillna(''))\n",
    "        articles_df[\"article_title_clean\"] = articles_df[\"article_title\"].str.lower()\n",
    "\n",
    "        return articles_df\n",
    "    \n",
    "    def clean_article_abstracts(self, articles_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Clean abstracts\n",
    "        articles_df = articles_df.assign(abstract=articles_df['abstract'].fillna(''))\n",
    "        articles_df[\"abstract_clean\"] = articles_df[\"abstract\"].str.lower()\n",
    "                \n",
    "        # Update 'abstract_clean' to an empty string if it contains fewer than 3 words\n",
    "        articles_df['abstract_clean'] = articles_df['abstract_clean'].apply(lambda x: '' if len(x.split()) < 3 else x)\n",
    "\n",
    "        return articles_df\n",
    "    \n",
    "    def clean_article_contents(self, contents_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Clean contents\n",
    "        contents_df = contents_df.assign(content=contents_df['content'].fillna(''))\n",
    "        contents_df['content_clean'] = contents_df['content'].str.lower()\n",
    "\n",
    "        return contents_df\n",
    "    \n",
    "    def clean_archive_titles (self, archives_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Clean titles\n",
    "        archives_df = archives_df.assign(archive_title=archives_df['archive_title'].fillna(''))\n",
    "        archives_df[\"archive_title_clean\"] = archives_df[\"archive_title\"].str.lower()\n",
    "\n",
    "        return archives_df\n",
    "\n",
    "class DataIngestion_MySQL:\n",
    "    \"\"\"\n",
    "    Class responsible for reading data files and performing initial data cleaning.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.DB_HOST = \"localhost\"\n",
    "        self.DB_USER = \"root\"\n",
    "        self.DB_PASSWORD = \"root\"\n",
    "        self.DB_NAME = \"fmdb\"\n",
    "        self.mysql_writer = MySQLWriter(self.DB_HOST, self.DB_USER, self.DB_PASSWORD, self.DB_NAME)\n",
    "\n",
    "    def load_data(self) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Loads the archives, articles, and contents DataFrames from MySQL\n",
    "        \"\"\"\n",
    "        print(\"[INFO] Loading data files...\")\n",
    "        volumes_df = self.mysql_writer.read_table_volumes()\n",
    "        archives_df = self.mysql_writer.read_table_archives(status=\"\")\n",
    "        articles_df = self.mysql_writer.read_table_articles(status=\"\")\n",
    "        contents_df = self.mysql_writer.read_table_contents()\n",
    "        authors_df = self.mysql_writer.read_table_authors()\n",
    "        authors_articles_df = self.mysql_writer.read_table_authors_articles()\n",
    "\n",
    "        return volumes_df, archives_df, articles_df, contents_df, authors_df, authors_articles_df\n",
    "\n",
    "    def clean_articles(self, articles_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Simple Data preprocessing.\n",
    "        \"\"\"\n",
    "        print(\"[INFO] Cleaning articles...\")\n",
    "        articles_df = articles_df.fillna('')\n",
    "\n",
    "        return articles_df\n",
    "    \n",
    "    def clean_article_authors(self, articles_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Clean authors\n",
    "        articles_df = articles_df.assign(author=articles_df['author'].fillna(''))\n",
    "        articles_df['author_clean'] = (articles_df['author']\n",
    "                                            .str.replace(r'\\.', '', regex=True)\n",
    "                                            .str.replace(r'\\s+', ' ', regex=True)  # This replaces multiple spaces with one\n",
    "                                            .str.strip()  # Remove leading/trailing spaces\n",
    "                                            )\n",
    "\n",
    "        return articles_df\n",
    "    \n",
    "    def clean_article_titles (self, articles_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Clean titles\n",
    "        articles_df = articles_df.assign(article_title=articles_df['article_title'].fillna(''))\n",
    "        articles_df[\"article_title_clean\"] = articles_df[\"article_title\"].str.lower()\n",
    "\n",
    "        return articles_df\n",
    "    \n",
    "    def clean_article_abstracts(self, articles_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Clean abstracts\n",
    "        articles_df = articles_df.assign(abstract=articles_df['abstract'].fillna(''))\n",
    "        articles_df[\"abstract_clean\"] = articles_df[\"abstract\"].str.lower()\n",
    "                \n",
    "        # Update 'abstract_clean' to an empty string if it contains fewer than 3 words\n",
    "        articles_df['abstract_clean'] = articles_df['abstract_clean'].apply(lambda x: '' if len(x.split()) < 3 else x)\n",
    "\n",
    "        return articles_df\n",
    "    \n",
    "    def clean_article_contents(self, contents_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Clean contents\n",
    "        contents_df = contents_df.assign(content=contents_df['content'].fillna(''))\n",
    "        contents_df['content_clean'] = contents_df['content'].str.lower()\n",
    "\n",
    "        return contents_df\n",
    "    \n",
    "    def clean_archive_titles (self, archives_df: pd.DataFrame) -> pd.DataFrame:\n",
    "        # Clean titles\n",
    "        archives_df = archives_df.assign(archive_title=archives_df['archive_title'].fillna(''))\n",
    "        archives_df[\"archive_title_clean\"] = archives_df[\"archive_title\"].str.lower()\n",
    "\n",
    "        return archives_df\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    Class responsible for additional text cleaning and preprocessing before topic modeling.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stop_words.update(['et', 'al']) # custom stop words\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def preprocess_text(self, text: str) -> str:\n",
    "        \"\"\"\n",
    "        Removes URLs, non-alphabetic characters, stopwords, casefolding, lemmatization.\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove URLs\n",
    "        text = re.sub(r'http\\S+|www\\S+', '', text, flags=re.MULTILINE)\n",
    "\n",
    "        # Remove non-alpha\n",
    "        # text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        \n",
    "        # Remove non-alpha except for certain special characters that may affect context\n",
    "        text = re.sub(r'[^a-zA-Z\\s.,!?;:\\'\\\"()\\[\\]-]', ' ', text)\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "        # words = text.split()\n",
    "        # Remove stopwords is not advised in BERTopic. \n",
    "        # https://maartengr.github.io/BERTopic/getting_started/tips_and_tricks/tips_and_tricks.html#document-length\n",
    "        # words = [w for w in words if w not in self.stop_words]\n",
    "        \n",
    "        # Lemmatize\n",
    "        # words = [self.lemmatizer.lemmatize(w) for w in words]\n",
    "        # text = ' '.join(words)\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def remove_stop_words (self, text: str) -> str:\n",
    "        # Remove stopwords is not advised in BERTopic. \n",
    "        # https://maartengr.github.io/BERTopic/getting_started/tips_and_tricks/tips_and_tricks.html#document-length\n",
    "\n",
    "        words = text.split()\n",
    "        words = [w for w in words if w not in self.stop_words]\n",
    "        text = ' '.join(words)\n",
    "\n",
    "        return text\n",
    "    \n",
    "    def lemmatize (self, text: str) -> str:\n",
    "        # Lemmatize\n",
    "\n",
    "        words = text.split()\n",
    "        words = [self.lemmatizer.lemmatize(w) for w in words]\n",
    "        text = ' '.join(words)\n",
    "\n",
    "        return text\n",
    "\n",
    "    # Translate to English if the language is not English\n",
    "    def translate_to_english (self, lang, text):\n",
    "        if lang != 'en':\n",
    "            translated_text = GoogleTranslator(source='auto', target='en').translate(text)\n",
    "            if translated_text is not None:\n",
    "                text = translated_text\n",
    "        return text\n",
    "    \n",
    "    def detect_language (self, text):\n",
    "        detected_lang = detect(text)\n",
    "        return detected_lang"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
