{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Scraping FirstMonday.com</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prerequisites:\n",
    "* Make sure to place chromedriver.exe in the same directory as your code, in \"\\chromedriver-win64\" subfolder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 00_lib_preprocessing.ipynb\n",
    "%run 00_lib_sqlwriter.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##### Web Scraper Using Selenium\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "import os\n",
    "import html\n",
    "\n",
    "class FirstMondayBot:\n",
    "    \"\"\"\n",
    "    A bot to automate the scraping of journals using Selenium.\n",
    "    \"\"\"\n",
    "    def __init__(self, driver, mysql_writer):\n",
    "        # Initializes the bot with a Selenium WebDriver instance and a MySQLWriter instance.\n",
    "        self.driver = driver\n",
    "        self.mysql_writer = mysql_writer\n",
    "\n",
    "    \n",
    "    def scrape_archives_urls (self, base_url, max_pages=1):\n",
    "        # Scrapes archives URLs and stores them in the 'archives' table.\n",
    "\n",
    "        current_page = 0\n",
    "\n",
    "        try:\n",
    "            # Navigate to the base URL\n",
    "            self.driver.get(base_url)\n",
    "            time.sleep(2)\n",
    "\n",
    "            while current_page <= max_pages:\n",
    "                current_page += 1\n",
    "                print(f\"Scraping page {current_page}...\")\n",
    "\n",
    "                # Wait for the issues_archive to be visible\n",
    "                WebDriverWait(self.driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.CLASS_NAME, \"issues_archive\"))\n",
    "                )\n",
    "\n",
    "                # Find and extract links\n",
    "                struct_item_container = self.driver.find_element(By.CLASS_NAME, \"issues_archive\")\n",
    "                post_links = struct_item_container.find_elements(By.XPATH, \".//a[@href]\")\n",
    "\n",
    "                # Collect unique archives URLs from the current page\n",
    "                base_urls = set()\n",
    "                for link in post_links:\n",
    "                    # archive_url, volume_number, archive_title, archive_publication_date, editor, import_date\n",
    "                    url = link.get_attribute(\"href\")\n",
    "                    volume_number = ''\n",
    "                    archive_title = ''\n",
    "                    archive_publication_date = '1900-01-01'\n",
    "                    editor = ''\n",
    "                    base_urls.add((url, volume_number, archive_title, archive_publication_date, editor, datetime.now(), ''))\n",
    "\n",
    "                # Convert the set of URLs to a DataFrame\n",
    "                if base_urls:\n",
    "                    archives_df = pd.DataFrame(list(base_urls), columns=['archive_url','volume_number', 'archive_title', 'archive_publication_date', 'editor', 'import_date', 'status'])\n",
    "                    \n",
    "                    # Insert into the database\n",
    "                    self.mysql_writer.insert_archives(archives_df)\n",
    "                    print(f\"Inserted {len(base_urls)} archives URLs from page {current_page}.\")\n",
    "\n",
    "                # Attempt to click the \"next\" button to proceed to the next page\n",
    "                try:\n",
    "                    next_button = WebDriverWait(self.driver, 10).until(\n",
    "                        EC.element_to_be_clickable((By.XPATH, \"//a[contains(@class, 'next')]\"))\n",
    "                    )\n",
    "                    self.driver.execute_script(\"arguments[0].click();\", next_button)\n",
    "\n",
    "                    # Wait for the new page content to load by checking the staleness of the previous container\n",
    "                    WebDriverWait(self.driver, 3).until(EC.staleness_of(struct_item_container))\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(\"No more pages or error clicking the 'next' button:\", e)\n",
    "                    break\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while scraping archives: {e}\")\n",
    "\n",
    "    \n",
    "    def scrape_archives (self):\n",
    "        try:\n",
    "            df = self.mysql_writer.read_table_archives(status=None)\n",
    "            \n",
    "            for _, row in df.iterrows():\n",
    "                archive_url = row['archive_url']\n",
    "                print('Scraping archive_url:', archive_url)\n",
    "\n",
    "                self.driver.get(archive_url)\n",
    "                time.sleep(2)\n",
    "\n",
    "                bsObj = BeautifulSoup(self.driver.page_source, 'lxml') # Get the page source and parse it with Beautiful Soup\n",
    "\n",
    "                volume_number = ''\n",
    "                archive_title = ''\n",
    "                archive_publication_date = '1900-01-01'\n",
    "                editor = ''\n",
    "\n",
    "                volume_number = bsObj.find(\"h1\").getText().strip()\n",
    "                # print('volume_number: ', volume_number)\n",
    "\n",
    "                # Find the <div> with class \"description\"\n",
    "                description_div = bsObj.find(\"div\", {\"class\": \"description\"})\n",
    "                # print('description:', description_div)\n",
    "\n",
    "                if description_div:\n",
    "                    # Extract title and editor\n",
    "                    paragraph = description_div.find(\"p\")\n",
    "                    # print('paragraph:', paragraph)\n",
    "                    \n",
    "                    if paragraph:\n",
    "                        try:\n",
    "                            description = paragraph.get_text(separator='\\n').strip()\n",
    "                            lines = description.split('\\n')\n",
    "                            # print('lines:', lines)\n",
    "\n",
    "                            archive_title = lines[0].strip()\n",
    "                            # print('archive_title:', archive_title)\n",
    "\n",
    "                            # Check if lines[1] contains \"edited by\" or \"co-edited by\" \n",
    "                            if any(keyword in description.lower() for keyword in [\"edited by\", \"co-edited by\"]): \n",
    "                                editor = lines[1].strip() \n",
    "                                editor = re.sub(r'Edited by|Co-edited by', '', editor, flags=re.IGNORECASE).strip() \n",
    "                                # print('editor:', editor)\n",
    "\n",
    "                        except Exception as e:\n",
    "                            print(\"An error occurred:\", e)\n",
    "                            continue\n",
    "\n",
    "                published_div = bsObj.find(\"div\", {\"class\": \"published\"})\n",
    "                if published_div:\n",
    "                    archive_publication_date = published_div.find(\"span\", {\"class\": \"value\"}).text.strip()\n",
    "                    # print('archive_publication_date:', archive_publication_date)\n",
    "\n",
    "                base_urls = set()\n",
    "                base_urls.add((archive_url, volume_number, archive_title, archive_publication_date, editor, datetime.now(), 'PENDING'))\n",
    "            \n",
    "                # Update archives table\n",
    "                archives_df = pd.DataFrame(list(base_urls), columns=['archive_url', 'volume_number', 'archive_title', 'archive_publication_date', 'editor', 'import_date', 'status'])\n",
    "                self.mysql_writer.update_archives(archives_df)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred:\", e)\n",
    "\n",
    "    \n",
    "    def scrape_articles_urls (self):\n",
    "        try:\n",
    "            df = self.mysql_writer.read_table_archives(status=\"PENDING\")\n",
    "            article_urls = set()\n",
    "            archive_urls = set()\n",
    "\n",
    "            for _, row in df.iterrows():\n",
    "                archive_url = row['archive_url']\n",
    "\n",
    "                self.driver.get(archive_url)\n",
    "                time.sleep(3)\n",
    "\n",
    "                bsObj = BeautifulSoup(self.driver.page_source, 'lxml') # Get the page source and parse it with Beautiful Soup\n",
    "\n",
    "                # article_url, article_title, doi, article_publication_date, author, archive_url, import_date\n",
    "                article_url = ''\n",
    "                article_title = ''\n",
    "                doi = ''\n",
    "                article_publication_date = '1900-01-01'\n",
    "                author = ''\n",
    "                keyword= ''\n",
    "                abstract = ''\n",
    "                content_url = ''\n",
    "\n",
    "                articles = bsObj.find_all(\"h3\", {\"class\": \"title\"})\n",
    "                # print (articles)\n",
    "                for article in articles:\n",
    "                    a_tag = article.find('a')\n",
    "                    if a_tag:\n",
    "                        article_url = a_tag['href']\n",
    "                        # print('article_url: ', article_url)\n",
    "                        article_urls.add((article_url, article_title, doi, article_publication_date, author, keyword, abstract, archive_url, content_url, datetime.now(), 'PENDING'))\n",
    "                        archive_urls.add((archive_url,'COMPLETED'))\n",
    "\n",
    "                articles_df = pd.DataFrame(list(article_urls), columns=['article_url', 'article_title', 'doi', 'article_publication_date', 'author', 'keyword', 'abstract', 'archive_url', 'content_url', 'import_date', 'status'])\n",
    "                self.mysql_writer.insert_articles(articles_df)\n",
    "\n",
    "                archives_df = pd.DataFrame(list(archive_urls), columns=['archive_url', 'status'])\n",
    "                self.mysql_writer.update_archives_status(archives_df) # update archive status to 'completed'\n",
    "                \n",
    "            # print(f\"Inserted article URL: {(article_urls)}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred:\", e)\n",
    "\n",
    "    \n",
    "    def scrape_articles (self):\n",
    "        try:\n",
    "            df = self.mysql_writer.read_table_articles(status=\"PENDING\")\n",
    "            article_urls = set()\n",
    "\n",
    "            for _, row in df.iterrows():\n",
    "                count=+1\n",
    "                article_url = row['article_url']\n",
    "                print('----------------Scraping article_url:', article_url, '----------------')\n",
    "\n",
    "                archive_url = row['archive_url']\n",
    "                # print('archive_url:', archive_url)\n",
    "\n",
    "                self.driver.get(article_url)\n",
    "                time.sleep(2)\n",
    "\n",
    "                bsObj = BeautifulSoup(self.driver.page_source, 'lxml') # Get the page source and parse it with Beautiful Soup\n",
    "                \n",
    "                # article_url, title, doi, publication_date, author, abstract, archive_url, import_date\n",
    "                article_title = ''\n",
    "                doi = ''\n",
    "                article_publication_date = '1900-01-01'\n",
    "                author = ''\n",
    "                keyword = ''\n",
    "                abstract = ''\n",
    "                content_url = ''\n",
    "                error = ''\n",
    "\n",
    "                if bsObj.find(\"div\", {\"class\": \"error-code\"}):\n",
    "                    error = (bsObj.find(\"div\", {\"class\": \"error-code\"}).get_text().strip())\n",
    "                    print ('error:', error)\n",
    "                    if error.lower() == 'http error 500':\n",
    "                        continue\n",
    "\n",
    "                # Find the <div> with class \"description\"\n",
    "                article_title = bsObj.find(\"h1\", {\"class\": \"page_title\"})\n",
    "                if article_title: article_title = article_title.get_text().strip()\n",
    "                print('article_title:', article_title)\n",
    "\n",
    "                doi_section = bsObj.find(\"section\", {\"class\": \"item doi\"})\n",
    "                if doi_section: doi = doi_section.find('a').get('href')\n",
    "                print('doi:', doi)\n",
    "\n",
    "                date_section = bsObj.find(\"div\", {\"class\": \"item published\"})\n",
    "                if date_section: date_section = date_section.find(\"span\")\n",
    "                if date_section: article_publication_date = date_section.get_text().strip()\n",
    "                article_publication_date = article_publication_date[:10]\n",
    "                print('article_publication_date:', article_publication_date)\n",
    "\n",
    "                author_section = bsObj.find(\"section\", {\"class\": \"item authors\"})\n",
    "                if author_section: author_section = author_section.find_all(\"span\",{\"class\":\"name\"})\n",
    "                if author_section:  author = [author.get_text().strip() for author in author_section]\n",
    "                author = ', '.join(author)\n",
    "                print('author:', author)\n",
    "\n",
    "                keyword_section = bsObj.find(\"section\", {\"class\": \"item keywords\"})\n",
    "                if keyword_section: \n",
    "                    keyword_section = keyword_section.find(\"span\",{\"class\":\"value\"})\n",
    "                    if keyword_section:  \n",
    "                        keyword_text = [keyword.get_text().strip() for keyword in keyword_section] \n",
    "                        if keyword_text: \n",
    "                            keyword_text = \", \".join(keyword_text)\n",
    "                            # Clean keyword text\n",
    "                            text_preprocessor = TextPreprocessor()\n",
    "                            keyword = text_preprocessor.preprocess_text(keyword_text)\n",
    "                print('keyword:', keyword)\n",
    "\n",
    "                abstract_section = bsObj.find(\"section\", {\"class\": \"item abstract\"})\n",
    "                if abstract_section:\n",
    "                    abstract = abstract_section.find(\"p\")\n",
    "                    if  abstract:\n",
    "                        abstract = abstract.get_text().strip()\n",
    "                    else:\n",
    "                        abstract = abstract_section.get_text(separator=\" \", strip=True).strip()\n",
    "                        abstract = re.sub(r'^\\babstract\\b', '', abstract, flags=re.IGNORECASE).replace('\\n', '')\n",
    "                print('abstract:', abstract)\n",
    "\n",
    "                a_tag = bsObj.find(\"a\", {\"class\": \"obj_galley_link file\"})\n",
    "                if a_tag: content_url = a_tag.get('href')\n",
    "                print('content_url:', content_url)\n",
    "\n",
    "                article_urls.add((article_url, article_title, doi, article_publication_date, author, keyword, abstract, archive_url, content_url, datetime.now(), 'COMPLETED'))\n",
    "\n",
    "                # Update archives table                \n",
    "                articles_df = pd.DataFrame(list(article_urls), columns=['article_url', 'article_title', 'doi', 'article_publication_date', 'author', 'keyword', 'abstract', 'archive_url', 'content_url', 'import_date', 'status'])\n",
    "                self.mysql_writer.update_articles(articles_df)\n",
    "                time.sleep(3)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred:\", e)\n",
    "\n",
    "    \n",
    "    def selectively_escape(self, text):\n",
    "        # List of characters to escape\n",
    "        problematic_chars = ['<', '>', '&', '\"', \"'\", '\\n', '\\r', '\\t']\n",
    "        escape_map = {\n",
    "            '<': '&lt;',\n",
    "            '>': '&gt;',\n",
    "            '&': '&amp;',\n",
    "            '\"': '&quot;',\n",
    "            \"'\": '&#39;',\n",
    "            '\\n': '&#10;',\n",
    "            '\\r': '&#13;',\n",
    "            '\\t': '&#9;',\n",
    "        }\n",
    "\n",
    "        # Escape the problematic characters in the text\n",
    "        for char in problematic_chars:\n",
    "            text = text.replace(char, escape_map[char])\n",
    "\n",
    "        return text\n",
    "    \n",
    "    \n",
    "    def scrape_contents(self):\n",
    "        try:\n",
    "            df = self.mysql_writer.read_table_articles_wo_contents()\n",
    "\n",
    "            for _, row in df.iterrows():\n",
    "                content_url = row['content_url']\n",
    "                print('----------------Scraping content_url:', content_url, '----------------')\n",
    "\n",
    "                # Navigate to the content URL using Selenium\n",
    "                self.driver.get(content_url)\n",
    "                time.sleep(5)  # Allow time for the page to load\n",
    "\n",
    "                # Wait for the iframe to be present and extract it using Selenium\n",
    "                iframe_element = WebDriverWait(self.driver, 10).until(\n",
    "                    EC.presence_of_element_located((By.TAG_NAME, \"iframe\"))\n",
    "                )\n",
    "                iframe_url = iframe_element.get_attribute(\"src\").strip()\n",
    "                print(\"iframe_url:\", iframe_url)\n",
    "\n",
    "                # Set headers to mimic a real browser request\n",
    "                headers = {\n",
    "                    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                                \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                                \"Chrome/111.0.0.0 Safari/537.36\",\n",
    "                    \"Referer\": content_url  # Some servers require a valid referer\n",
    "                }\n",
    "\n",
    "                # Fetch the iframe content using requests with headers\n",
    "                iframe_response = requests.get(iframe_url, headers=headers)\n",
    "                if iframe_response.status_code != 200:\n",
    "                    print(f\"Failed to retrieve iframe content from {iframe_url} with status code {iframe_response.status_code}\")\n",
    "                    continue\n",
    "\n",
    "                # Parse the iframe HTML content with BeautifulSoup\n",
    "                try:\n",
    "                    iframe_soup = BeautifulSoup(iframe_response.text, \"lxml\")\n",
    "                except Exception:\n",
    "                    print(\"lxml not available, using default HTML parser.\")\n",
    "                    iframe_soup = BeautifulSoup(iframe_response.text, \"html.parser\")\n",
    "                iframe_text = iframe_soup.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "                # Escape any special characters (like <, >, &, etc.) in the iframe text\n",
    "                # iframe_text_cleaned = self.selectively_escape(iframe_text)\n",
    "                \n",
    "                # Insert to database\n",
    "                contents_df = pd.DataFrame([[content_url, iframe_url, iframe_text]], columns=['content_url', 'iframe_url', 'content'])\n",
    "                self.mysql_writer.insert_contents(contents_df)\n",
    "\n",
    "                time.sleep(3)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"An error occurred during scraping contents:\", e)\n",
    "\n",
    "    \n",
    "    def close(self):\n",
    "        # Closes the Selenium WebDriver instance.\n",
    "        self.driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'volumes' is ready.\n",
      "Table 'archives' is ready.\n",
      "Table 'articles' is ready.\n",
      "Table 'contents' is ready.\n",
      "Table 'authors' is ready.\n",
      "Table 'authors_articles' is ready.\n",
      "----------------Scraping content_url: https://firstmonday.org/ojs/index.php/fm/article/view/828/737 ----------------\n",
      "iframe_url: https://firstmonday.org/ojs/index.php/fm/article/download/828/737?inline=1\n",
      "lxml not available, using default HTML parser.\n",
      "Inserted article URL: https://firstmonday.org/ojs/index.php/fm/article/view/828/737\n",
      "MySQL connection closed.\n"
     ]
    }
   ],
   "source": [
    "# ##### Main Execution: Initializing and Running the Scraper\n",
    "\n",
    "import os\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "# Setup Chrome options for Selenium WebDriver\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless=new\")\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--disable-software-rasterizer\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) \"\n",
    "                            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "                            \"Chrome/111.0.0.0 Safari/537.36\")\n",
    "\n",
    "# Initialize the WebDriver\n",
    "chrome_driver_path = os.path.join(os.getcwd(), \"chromedriver-win64\", \"chromedriver.exe\")\n",
    "service = Service(chrome_driver_path)\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Define login parameters\n",
    "URL = \"https://firstmonday.org/ojs/index.php/fm/issue/archive\"\n",
    "\n",
    "# MySQL Connection Details\n",
    "DB_HOST = \"localhost\"\n",
    "DB_USER = \"root\"\n",
    "DB_PASSWORD = \"root\"\n",
    "DB_NAME = \"fmdb\"\n",
    "\n",
    "# Initialize the MySQLWriter instance\n",
    "mysql_writer = MySQLWriter(DB_HOST, DB_USER, DB_PASSWORD, DB_NAME)\n",
    "\n",
    "# Create an instance of FirstMondayBot\n",
    "bot = FirstMondayBot(driver, mysql_writer)\n",
    "\n",
    "try:\n",
    "    # Scrape archives' URLs and store them in the 'archives' table\n",
    "    BASE_FORUM_URL = \"https://firstmonday.org/ojs/index.php/fm/issue/archive/\"\n",
    "    MAX_PAGES = 50  # Max page 50\n",
    "\n",
    "    # # print(\"1. Inserting archive URLs...\")\n",
    "    # bot.scrape_archives_urls (BASE_FORUM_URL, MAX_PAGES)\n",
    "\n",
    "    # # Scrape archives and store them in the 'archives' table\n",
    "    # print(\"2. Updating archives...\")\n",
    "    # bot.scrape_archives()\n",
    "\n",
    "    # # print(\"3. Inserting article URLs...\")\n",
    "    # bot.scrape_articles_urls()\n",
    "\n",
    "    # # print(\"4. Updating articles...\")\n",
    "    # bot.scrape_articles()\n",
    "\n",
    "    # print(\"5. Inserting html contents...\")\n",
    "    bot.scrape_contents()\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"An error occurred during scraping:\", e)\n",
    "\n",
    "finally:\n",
    "    # Close the WebDriver and the MySQL connection\n",
    "    bot.close()\n",
    "    mysql_writer.close_connection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bertopic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
